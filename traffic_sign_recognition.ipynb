{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nFeRtpkpSOj"
      },
      "outputs": [],
      "source": [
        "# Step 1: Data Acquisition\n",
        "# Download the dataset file(GTSRB - German Traffic Sign Recognition Benchmark from Kaggle)\n",
        "\n",
        "# Install kaggle\n",
        "!pip install kaggle\n",
        "\n",
        "# Mount the Google drive so that you can store your Kaggle API credentials\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a Kaggle directory\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Copy the kaggle.json file to the Kaggle directory\n",
        "!cp /content/drive/MyDrive/Kaggle/kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set permissions for the Kaggle API key\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the dataset using the Kaggle API\n",
        "!kaggle datasets download -d meowmeowmeowmeowmeow/gtsrb-german-traffic-sign\n",
        "\n",
        "# Unzip the dataset (if needed)\n",
        "!unzip gtsrb-german-traffic-sign.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "h59ENKnYveZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f92faf-2cb5-4943-d285-36252bc7653d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training images: 39209\n",
            "Number of training labels: 39209\n",
            "Number of test images: 12630\n",
            "Number of test labels: 12630\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Data Preprocessing\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def load_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Load the corresponding CSV file based on data_dir\n",
        "    csv_file = os.path.join(f'{data_dir}.csv')  # Construct CSV filename\n",
        "    data_df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Get the image file paths from the CSV\n",
        "    image_paths = data_df['Path'].tolist()\n",
        "\n",
        "    # Iterate over image paths and load images with corresponding labels\n",
        "    for i, image_path in enumerate(image_paths):\n",
        "        full_image_path = os.path.join(image_path) # Construct full image path\n",
        "        image = cv2.imread(full_image_path)\n",
        "        if image is not None:\n",
        "            image = cv2.resize(image, (32, 32))\n",
        "            images.append(image / 255.0)\n",
        "            labels.append(data_df.loc[i, 'ClassId'])  # Extract label using index i from 'ClassId' column\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "train_images, train_labels = load_data('Train')\n",
        "test_images, test_labels = load_data('Test')\n",
        "print(f\"Number of training images: {len(train_images)}\")\n",
        "print(f\"Number of training labels: {len(train_labels)}\")\n",
        "print(f\"Number of test images: {len(test_images)}\")\n",
        "print(f\"Number of test labels: {len(test_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding sign descriptions corresponding to sign ids of 'meta.csv' file\n",
        "\n",
        "sign_descriptions = {\n",
        "    \"1.32\": \"Pedestrian crossing\",\n",
        "    \"3.29\": \"Speed limit (20km/h)\",\n",
        "    \"3.27\": \"Speed limit (100km/h)\",\n",
        "    \"1.22\": \"Right-of-way at the next intersection\",\n",
        "    \"2.3\": \"Priority road\",\n",
        "    \"2.1\": \"Yield\",\n",
        "    \"2.2\": \"Stop\",\n",
        "    \"3.1\": \"No entry\",\n",
        "    \"3.3\": \"No vehicles\",\n",
        "    \"3.21\": \"End of no passing\",\n",
        "    \"1.39\": \"Traffic signals\",\n",
        "    \"1.2\": \"Dangerous curve to the right\",\n",
        "    \"1.1\": \"Dangerous curve to the left\",\n",
        "    \"1.3.2\": \"Double curve (first to the right)\",\n",
        "    \"1.13\": \"Slippery road\",\n",
        "    \"1.5.2\": \"Road narrows on the right\",\n",
        "    \"1.37\": \"Crosswind\",\n",
        "    \"1.24\": \"Road work\",\n",
        "    \"1.33\": \"Right-hand curve\",\n",
        "    \"1.34\": \"Left-hand curve\",\n",
        "    \"1.36\": \"Traffic merges from the right\",\n",
        "    \"3.42\": \"End of all speed and passing limits\",\n",
        "    \"4.2\": \"Mandatory direction (straight or right)\",\n",
        "    \"4.3\": \"Mandatory direction (straight or left)\",\n",
        "    \"4.1\": \"Mandatory direction (straight)\",\n",
        "    \"4.4\": \"Mandatory direction (right)\",\n",
        "    \"4.5\": \"Mandatory direction (left)\",\n",
        "    \"4.7\": \"Roundabout\",\n",
        "    \"4.8\": \"Mandatory path for bicycles\",\n",
        "    \"3.26\": \"End of speed limit (80km/h)\",\n",
        "    \"3.28\": \"End of prohibition of overtaking\",\n",
        "    \"3.25\": \"End of speed limit (60km/h)\",\n",
        "    \"3.3\": \"No vehicles\",\n",
        "}\n",
        "\n",
        "# Load the Meta.csv file\n",
        "meta_df = pd.read_csv('Meta.csv')\n",
        "\n",
        "# Add the new column with sign descriptions(By converting 'SignId' column to 'str')\n",
        "meta_df['SignDescription'] = meta_df['SignId'].astype(str).map(sign_descriptions)\n",
        "\n",
        "# Save the updated DataFrame\n",
        "meta_df.to_csv('meta_updated.csv', index=False)"
      ],
      "metadata": {
        "id": "WlgLmVjsQsvJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CfHhZKAwxKM"
      },
      "outputs": [],
      "source": [
        "# Step 3 : Design a CNN architecture\n",
        "# Define a CNN model using Tensorflow/Keras\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models # layers provides access to various neural network layers, models allow for model creation\n",
        "\n",
        "# Create a sequential model where each layer has one input tensor and one output tensor\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(43, activation='softmax')  # 43 classes for traffic signs\n",
        "])\n",
        "\n",
        "# 1. Convolutional Layer: This adds a 2D convolutional layer with 32 filters (or kernels), each of size 3 × 3 3×3. The activation function used is ReLU (Rectified Linear Unit), which introduces non-linearity by outputting the input directly if it is positive; otherwise, it outputs zero. The input_shape parameter specifies the shape of the input images, which are grayscale images of size 28 × 28 28×28 pixels.\n",
        "# 2. Max Pooling Layer: This layer downsamples the feature maps by taking the maximum value in each 2 × 2 2×2 window. This reduces the spatial dimensions of the feature maps by half, helping to decrease computational load and control overfitting by providing an abstracted representation of the features.\n",
        "# 3. Second Convolutional Layer: Similar to the first convolutional layer but with 64 filters. This layer learns more complex features from the pooled outputs of the previous layer.\n",
        "# 4. Second Max Pooling Layer: Again, this layer further reduces the size of the feature maps by taking the maximum value in 2 × 2 2×2 windows from the output of the last convolutional layer.\n",
        "# 5. Flatten Layer: This layer converts the multi-dimensional output from the previous layer into a one-dimensional vector. It prepares the data for the fully connected (Dense) layers that follow. For example, if the output from the last pooling layer is 3 × 3 × 64 3×3×64, it will be flattened to a vector of length 576 576.\n",
        "# 6. Dense Layer: A fully connected layer with 128 neurons. Each neuron receives input from all neurons in the previous layer. The ReLU activation function is again used here to introduce non-linearity.\n",
        "# 7. Output Layer: This is another Dense layer but with 43 neurons corresponding to the number of classes in the traffic sign dataset. The softmax activation function is used here to convert raw logits into probabilities that sum to one across all classes. This allows for multi-class classification where each output neuron represents a class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5Lz-h6nUxO4z"
      },
      "outputs": [],
      "source": [
        "# Step 4: Compile the Model\n",
        "# Compile the model with an appropriate optimizer and loss function\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 'Adam' optimizer, refers to adaptive moment estimation algorithm used,\n",
        "# The loss function used during training is 'sparse_categrical_crossentropy'\n",
        "# 'Accuracy' is the metric specified to be evaluated during training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUM2vbfMx5wI"
      },
      "outputs": [],
      "source": [
        "# Step 5: Train and validate the model\n",
        "\n",
        "# 5.1: Train the model\n",
        "# Split the dataset into training and validation sets(eg: using 'train_test_split' from sklearn) and train the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size = 0.2)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# It splits the original dataset into a training set and a validation set using an 80-20 split.\n",
        "# It trains the Keras model on the training set for 10 epochs while validating its performance on the validation set after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcjaJ4DE1chD"
      },
      "outputs": [],
      "source": [
        "# Step 6: Evaluating the model\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print(f'Test accuracy: {test_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbcf1WC81oaZ"
      },
      "outputs": [],
      "source": [
        "# Step 7: Testing the model using an uploaded image\n",
        "from google.colab import files\n",
        "from IPython.display import Image, display, clear_output\n",
        "import ipywidgets as widgets\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "# Creating an upload button\n",
        "uploader = widgets.FileUpload(\n",
        "    accept='image/*',  # Accept only image files\n",
        "    multiple=False      # Do not allow multiple files\n",
        ")\n",
        "\n",
        "display(uploader)\n",
        "\n",
        "# Function that preprocess the uploaded image and uses the model to make predictions\n",
        "def load_and_preprocess_image(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (32, 32))  # Resize to match model input shape\n",
        "    img = img / 255.0  # Normalize the image\n",
        "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "    return img\n",
        "\n",
        "def predict_uploaded_image(model, uploader):\n",
        "    if uploader.value:\n",
        "        # Retrieve the uploaded file\n",
        "        [uploaded_file] = uploader.value.values()\n",
        "\n",
        "        # Get the content and the name of the uploaded file\n",
        "        content = uploaded_file['content']\n",
        "        name = uploaded_file['metadata']['name']\n",
        "\n",
        "        # Write the content to a file\n",
        "        with open(name, 'wb') as f:\n",
        "            f.write(content)\n",
        "\n",
        "        # Preprocess the image and make predictions\n",
        "        img_array = load_and_preprocess_image(name)\n",
        "        prediction = model.predict(img_array)\n",
        "        predicted_class = np.argmax(prediction)\n",
        "\n",
        "        # Get the sign description from the CSV file\n",
        "        sign_description = meta_df[meta_df['ClassId'] == predicted_class]['SignDescription'].values\n",
        "        sign_description = sign_description[0] if len(sign_description) > 0 else \"Description not found\"\n",
        "\n",
        "        # Display the uploaded image and the prediction result\n",
        "        pil_img = PILImage.open(name)\n",
        "        pil_img.thumbnail((150, 150))  # Resize the image to 150x150 pixels\n",
        "        display(pil_img)\n",
        "\n",
        "        print(f'The model predicts that the image is of class value: {predicted_class}. {sign_description}')\n",
        "\n",
        "        # Clear the uploader for the next upload\n",
        "        uploader.value.clear()\n",
        "\n",
        "# Creating a button for prediction\n",
        "\n",
        "button = widgets.Button(description=\"Predict\")\n",
        "output = widgets.Output()\n",
        "\n",
        "# Define the button click event\n",
        "def on_button_clicked(b):\n",
        "    with output:\n",
        "        clear_output(wait=True)  # Clear previous output\n",
        "        predict_uploaded_image(model, uploader)\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "# Display button and output area\n",
        "display(button, output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}